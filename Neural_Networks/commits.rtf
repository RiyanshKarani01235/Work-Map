{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl-320\sa160\pardirnatural

\f0\fs30 \cf0 COMMIT 1:\
Neural Networks And Deep Learning
\f1\fs24 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl-240\pardirnatural

\f0\fs20 \cf0 This Website is basically a book, regarding to Neural Networks and deep\
learning (http://neuralnetworksanddeeplearning.com/index.html), written\
by Michael Nielsen, and consists of a lot of basic material, explained\
in excruciating detail and clarity. Must be read, because it will give\
you a feel for the entire subject, as well as a roadmap for studying\
the subject.\
\
1) Read only the first three chapters as they contain all the working\
knowledge required to understand the basics and the applications of\
neural networks. The next two chapters are a bit technical, and should\
be read after covering the contents of the first three chapters in\
greater detail\
2) Write down working programs for Chapter 1 and chapter 2, as these\
two chapters form the crux of the subject, and should be understood\
completely.\
3)Understanding the Contents of Chapter 1 and 2 is essential, and\
should be understood with immense clarity.\
4)Go through the programs in chapter 3 loosely, since this chapter\
points out various downfalls of the previous code, suggests further\
optimisation algorithms and readings. The contents of this chapter is\
important from a subjective point of view, since it will provide a\
roadmap for future study, but the programs should be kept for tackling\
later.
\fs30 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl-320\sa160\pardirnatural
\cf0 \
COMMIT 2 :\
Cauchy-Schwarz inequality
\f1\fs24 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl-240\pardirnatural

\f0\fs20 \cf0 While going through the chapter 1 of the book Neural Networks and deep\
learning by Michael Nielsen (link provided in the previous commit),\
there is a portion where Cauchy-Schwarz inequality is used to prove\
that using gradient descent algorithm will always decrease the value of\
the cost function. Here are the links which i referred to for\
understanding the Cauchy-Schwarz inequality.\
\
1)wikipedia(this link explains the basic form and gives a short proof\
for the Cauchy-Schwarz inequality) :\
https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality\
\
2)Khan academy(this video gives a complete proof of the Cauchy-Schwarz\
inequality, and explains the use of this theorem) :\
https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/dot_c\
ross_products/v/proof-of-the-cauchy-schwarz-inequality\
\
After referring to these two links (that is if you aren\'92t already\
familiar with it), you should try to solve the exercise given in the\
book, to prove that the gradient descent algorithm always reduces the\
value of the quadratic cost function by using the Cauchy-Schwarz\
inequality\
\
\

\fs30 COMMIT 3 :\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl-320\sa160\pardirnatural
\cf0 Neural Networks and Machine Learning
\f1\fs24 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl-240\pardirnatural

\f0\fs20 \cf0 I have started studying the subject of neural networks, after going\
through the book mentioned in the earlier commit, by referring the book\
- Neural networks and machine Learning (3rd Edition) by Simon Haykin.\
(PDF LINK :\
http://www.mif.vu.lt/~valdas/DNT/Literatura/Haykin09/Haykin09.pdf)\
(Though I really advice you that you get a hard copy because you will\
have to spend a lot of time (and I mean A LOT) with this book)\
\
This book consists of a lot of subject matter, and I am eventually\
planning to study this book along with the MIT\'92s machine learning\
course 6.867, which covers somewhat similar subject matter (but for\
now, for the basics, I\'92ll just stick with Simon Haykin).\
\
This book is a little tedious, and requires a lot of understanding and\
searching a lot of places for complementary material, since a lot of\
things are considered to be prerequisites and thus only mentioned or\
explained vaguely.\
So, I will keep posting links, as a supplement material for the\
portions of the book that I found difficult to understand personally.\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl-320\sa160\pardirnatural

\fs30 \cf0 COMMIT 4 :\
Demonstration of Perceptron Convergence Theorem\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl-240\pardirnatural

\fs20 \cf0 This youtube video demonstrates the working of basic Perceptron\
Network, implementing the Perceptron convergence algorithm, to classify\
a linearly separable dataset into two classes.\
https://www.youtube.com/watch?v=vGwemZhPlsA\
\
After reading the perceptron convergence theorem in the book Neural\
Networks and Learning Machines - Simon Haykin, you should clearly\
understand what is going on in this video, and if not, you should (as I\
did) watch this video at a slower speed and try to understand what\
exactly is going on.\
}